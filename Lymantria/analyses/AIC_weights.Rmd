---
title: "Analysis Template"
author: ""
date: '`r format(Sys.Date(), "%B %d, %Y")`'
graphics: yes
output:
  github_document:
    toc: yes
  html_document:
    keep_md: yes
    theme: readable
  html_notebook:
    code_folding: hide
    theme: readable
editor_options:
  chunk_output_type: console
---

```{r setup, include = F}
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# @@@@@ Knitr Options
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

# Set root directory to the project directory
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())


# Set default knitr options: 
# Suppress warnings and messages, cache chunks, 
#  set default figure size to 6x8 at 300 dpi, and save a png and pdf
knitr::opts_chunk$set(warning = F, message = F, collapse = T, cache = T,
    fig.height = 6, fig.width = 8, dpi = 300, # 6x8" @ 300dpi:1800x2400=4.3MP
    dev = c('png', 'pdf'), dev.args = list(pdf = list(onefile = F)))

```



# Overview

Quabin and the CT FEN data sets each result in different model sets. This analysis explores whether we can use model weights and variable importance to get aggregated conclusions from the analysis of the two.

# Summary of Results

* The two data sets result in totally different model ranks - that is partly because Quabin has a clear winning model (with a weight of >90%) while the FEN data's best model only has a weight of 15%. 


 

```{r Main_Code, include = F, cache = F}

# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# @@@@@ Setup - This code is run, but output is hidden
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

# Load Packages
library(tidyverse) # Needed for data wrangling: dplyr, tidyr, ggplot2
library(cowplot) # Needed for publication-quality ggplots
library(ggthemes) # Needed for tufte_theme

# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# @@@@@ Data Preparation
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

# Import datasets
#data <- read.csv("data/file.csv")

# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# @@@@@ ggPlot Theme
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

ggplot.theme <- theme(
    
    # Text size for axis ticks
    axis.text.y = element_text(size = 14),
    axis.text.x = element_text(size = 14),
    
    # Text size for axis labels
    # Also move them away from the axes a bit for more space
    axis.title.x = element_text(size = 18, face = "bold", vjust = -1),
    axis.title.y = element_text(size = 18, face = "bold", vjust = 1.5),
    
    # Plot title size
    plot.title = element_text(size = 20, face = "bold"),
    
    # Margins for top, right, bottom, left
    plot.margin = grid::unit(c(1.5, 1.5, 1.5, 1.2), "lines"), 
    
    # Legend text size
    legend.text = element_text(size = 14),
    legend.text.align = 0, 
    legend.title = element_text(size = 16, face = "bold"),
    legend.key.size = grid::unit(1.4, "line"),
    legend.key = element_blank()
    )
theme_set(theme_tufte() + ggplot.theme)

```


```{r Data_Wrangling, echo = F, comment = ""}

# Put data wrangling code here
quab <- read.csv("data/QuabAICv3_0.csv")
ct17 <- read.csv("data/CTBurlap32ModelsAIC_2017.csv")

both <- left_join(quab, ct17, by="baseline", suffix = c("q", "c"))

labs <- str_split(both$baseline, pattern="_", simplify=TRUE)[,-1] %>% as.data.frame()
both <- bind_cols(mutate(labs, base = paste(V2, V3, sep="-")) %>% select(-V2, -V3) %>% 
    rename(metric=V1, harmonic=V4, period=V5), 
    both)

```

# Analyses

## Weights comparison

Plotting the weights from the two data sets. There is little correlation between them.

```{r quab_ct17_wtcomp, echo = F}
both %>% ggplot(., aes(x=weightc, y=weightq)) + geom_point() + theme_bw()

cor(both$weightc, both$weightq) ## negative correlation!
cor(both$weightc, both$weightq, method="spearman") ## ranks are slightly correlated though
```

Overall, it looks to me like we are on a hiding to nothing in trying to get an average across the two here. They are so different that I think our conclusion has to be that there is no one size fits all (boo). That seems to be a change from the last time I looked at these results in detail, but perhaps I was just not seeing the patterns.

## Variable importance
We can calculate the importance of each variable by adding up the weights of the models that contain it. Here, we can do that by adding up within categores - i.e. compare the sum of weights of models with NDVI with TCG (etc).

```{r variable_importance, echo = F}
both <- filter(both, metric != "reanalysis")

metric_comp <- both %>% group_by(metric) %>% 
  summarize(imp_q = sum(weightq), imp_c=sum(weightc)) %>% 
  arrange(desc(imp_q), desc(imp_c))

harmonic_comp <- both %>% group_by(harmonic) %>% 
  summarize(imp_q = sum(weightq), imp_c=sum(weightc)) %>% 
  arrange(desc(imp_q), desc(imp_c))

period_comp <- both %>% group_by(period) %>% 
  summarize(imp_q = sum(weightq), imp_c=sum(weightc)) %>% 
  arrange(desc(imp_q), desc(imp_c))

base_comp <- both %>% group_by(base) %>% 
  summarize(imp_q = sum(weightq), imp_c=sum(weightc)) %>% 
  arrange(desc(imp_q), desc(imp_c))

knitr::kable(metric_comp)
knitr::kable(harmonic_comp)
knitr::kable(period_comp)
knitr::kable(base_comp)
```


This analysis provides a little more insight into why there are disagreements between the models, but it is still hard to draw general conclusions. For Quabin, it appears that the best model combines TCG, h13, 2015 and full - which is no surprise given that is what the best model includes and that model is the best by a long way (and explains about 60% of the variation using just the fixed effects). The FEN data are less clear, which is perhaps unsurprising given that the variable we are predicting (defoliator abundance) is a little more removed from the predictor (defoliation). I would probably interpret this as that there is no clear "right" or "wrong" way of measuring gypsy moth abundance from the ground - they all do okay ($R^2 = 0.31$  for the worst performing model and $0.45$ for the best). 

However, we can say this about the individual metrics. NDVI performs poorly in both sites - do not use it could be a recommendation, although that has to be tempered by the 2018 results. The other decisions seem less impactful.

One thought I had looking at the aggregate results was that the difference between CT-FEN and Quabin might partly reflect the larger spatial scope, and hence heterogeniety, of the CT project. There is no "one-size-fits-all" solution when looking at a variety of forests.

So in terms of recommendations
* There is no single method that works best in all cases. However, the choice is not critical to the measures being useful.
* Perhaps avoid NDVI as a metric, although if that is all you have, that isn't the end of the world.
* If you are analysing data from a wide variety of sites, then you might want to use a bunch of different metrics.




## Session Information

```{r Session_Info, echo = F, comment = ""}

# Add session information to help with reproduceability
sessionInfo()


```


